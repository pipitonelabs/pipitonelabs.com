---
title: Deploying My Astro Blog in a Self-Hosted Kubernetes HomeLab
description: 'A comprehensive guide to deploying an Astro blog in a self-hosted Kubernetes HomeLab using GitHub Actions and Renovate.'
publishDate: 2025-11-29
updatedDate: 2025-12-01
tags:
  - Kubernetes
  - Astro
  - GitHub Actions
  - Renovate
  - Flux
  - DevOps
  - Homelab
heroImage: { src: './k8s.png', color: '#1567c5ff' }
draft: true
enableComment: true
---

Deploying this Astro-based blog to a self-hosted Kubernetes cluster has been a fascinating deep dive into understanding how modern web frameworks actually work under the hood. What started as wanting to containerize a simple blog became an exploration of Astro's TypeScript integration and server-side rendering capabilities. As I've discovered, Astro's flexibility with different adapters (from Vercel's serverless functions to standalone Node.js) reveals how frameworks give developers the choice to control their deployment strategy. 

In this post, I'll walk you through my continuous deployment setup from code commits to public access via `blog.pipitonelabs.com`, and how I was able to get the solution to function in a containerized environment.

GitHub Actions builds container images which my homelab Kubernetes cluster then pulls, deploys, updates, and exposes securely. Here's the high-level flow:

1. **Code Changes**: Push to the main branch triggers automated builds and releases.
2. **Dependency Updates**: Renovate detects package updates and creates pull requests.
3. **Deployment**: Flux (via HelmRelease) deploys new images automatically.
4. **Networking**: External-DNS creates DNS records, Envoy Gateway exposes the service, and Cloudflare Tunnel secures external access.

## Continuous Integration with GitHub Actions

The heart of my CI/CD pipeline is the [build-k8s.yaml](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.github/workflows/build-k8s.yaml) GitHub Actions workflow. It uses semantic versioning to create releases based on commit messages, and builds & pushes the package to my GitHub container registry. There's some additional logic to add release details, depending on the commit type.

### Key Features:
- **Semantic Release**: Analyzes commit messages (e.g., `feat:`, `fix:`) to determine version bumps. Configured in [.releaserc.json](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.releaserc.json).
- **Docker Build**: Creates multi-platform images and pushes to GitHub Container Registry (GHCR) using the multi-stage [Dockerfile](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/Dockerfile).
- **Tagging Strategy**: Generates version tags like `v1.2.3`, `v1.2`, `v1`, and SHA-based tags for flexibility.

### Semantic Release in Action

Semantic release analyzes conventional commit messages per [.releaserc.json](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.releaserc.json):

| Commit | Example | Bump | Tags |
|--------|---------|------|------|
| feat | `feat: add dark mode` | minor | v1.2.0, v1.2, v1 |
| fix/perf/revert/refactor | `fix: resolve login bug` | patch | v1.1.1, v1.1, v1 |
| docs/style/chore/etc | `docs: update readme` | none | SHA only |

The workflow runs on pushes to `main` or can be manually triggered. It outputs image metadata for easy consumption by deployment tools.

```yaml title=".github/workflows/build-k8s.yaml"
jobs:
  release:
    name: Semantic Release & Build Image
    runs-on: ubuntu-latest
    outputs:
      new_release_published: ${{ steps.semantic.outputs.new_release_published }}
      new_release_version: ${{ steps.semantic.outputs.new_release_version }}
  build-and-push:
    name: Build and Push Docker Image
    needs: release
    if: needs.release.outputs.new_release_published == 'true'
```

This ensures only meaningful changes trigger new deployments, reducing unnecessary builds.

## My Dockerfile Journey: From Slow Builds to Lightning-Fast Caching

When I first started containerizing my Astro blog, my builds were taking forever - probably 5+ minutes every time I made a tiny change to a file, or a blog post. That's when I realized I needed to understand how Docker caching actually works.

### Why Order Matters

Docker layers are like a stack of pancakes. Each instruction adds a new layer on top, and Docker is pretty smart about reusing layers when nothing has changed above them. But if you change something near the top, everything below gets rebuilt.

So I started with this simple question: "What changes most rarely in my project?"

The answer was obvious:
1. **Base system stuff** (Node, build tools) - rarely changes
2. **Dependencies** (package.json, bun.lock) - changes occasionally 
3. **My actual code** - changes constantly

By structuring my Dockerfile around this reality, I went from 5-minute rebuilds to 30-second builds for most changes. It sounds simple, but this insight completely transformed my workflow.

### The Build Stages Breakdown

Think of my Dockerfile as having four distinct stages, each serving a specific purpose:

**Base Stage**: This is where I install all the heavy lifting stuff that rarely changes - the Python and build tools needed for Sharp (my image processing library). Once this builds, it stays cached for weeks.

**Dependencies Stage**: I copy just the package files first (`package.json`, `bun.lock`) and run `bun install`. This means when I update a dependency, only this stage and everything after it rebuilds.

**Build Stage**: This is where the magic happens - I copy in all my source code, modify the Astro config for Kubernetes (more on that in a sec), and run the build. This stage rebuilds most frequently but is still pretty fast.

**Runtime Stage**: This creates the final, lean image with just Alpine Linux and the essential runtime files. Small, secure, and production-ready.

### The Performance Numbers Don't Lie

| What I Changed          | Before Optimization | After Caching |
|----------------------|----------------------|---------------|
| A CSS tweak          | Full rebuild (5m)    | 30 seconds    |
| New npm package      | Full rebuild (5m)    | 1 minute      |
| Base image update    | Full rebuild         | Full rebuild  |

### The Tweaks That Made Everything Click

**Sharp (Image Processing) Was a Beast**
Sharp needed some serious build tools during compilation, but only needed the lightweight runtime library later. So I split this across stages - build tools in the base stage, runtime `vips` in the final stage. The `SHARP_IGNORE_GLOBAL_LIBVIPS=1` environment variable was a game-changer too - it tells Sharp to use its own bundled libraries instead of fighting with system versions.

**The Adapter Switch Trick**
Here's something I'm pretty proud of. My blog uses Vercel's adapter locally (for easy development), but I needed to switch to a standalone Node adapter for Kubernetes. Instead of maintaining two different config files, I used a clever sed command during the build:

```dockerfile
RUN sed -i '2a import node from "@astrojs/node";' astro.config.ts && \
    sed -i 's/adapter: vercel(),/adapter: node({ mode: "standalone" }),/' astro.config.ts && \
    bun add @astrojs/node
```

This means one codebase works everywhere. No branches, no environment-specific configs, no headaches.

**Security Didn't Feel Like a Burden**
I built security into the process instead of bolting it on at the end. The `.dockerignore` file keeps sensitive stuff out of the build (no accidentally copying `.env` files or git history), and the final stage runs as a non-root user. Kubernetes enforces this further with pod security contexts.

The result? A 90% reduction in build times, smaller images, and a setup that's actually secure by default.


## Additional Production Hardening

In addition to the Dockerfile's built-in security (non-root user, `.dockerignore`), Kubernetes PodSecurityContext enforces:

```yaml title="Pod Security Context"
securityContext:
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  fsGroup: 1001
  seccompProfile:
    type: RuntimeDefault
```

Security should be a primary concern in production containerization. Here are the critical practices I've implemented:

### .dockerignore File

A `.dockerignore` file prevents unnecessary files from being copied into the build context, reducing image size and attack surface:

```gitignore title=".dockerignore"
# Dependencies and build artifacts
node_modules/
dist/
.cache/

# Environment and secrets
.env*
!.env.example

# Git and CI/CD
.git/
.github/
.gitlab-ci.yml

# Documentation and development files
README.md
CHANGELOG.md
.vscode/
.prettierrc

# OS and editor files
.DS_Store
Thumbs.db
*.swp
```

**Benefits**:
- **Faster builds** - Less data to transfer and process
- **Smaller images** - Excludes unnecessary development files
- **Security** - Prevents accidentally including secrets or sensitive files

### Non-Root User Implementation

Running containers as root is a security anti-pattern. The runtime stage creates a dedicated user:

```dockerfile title="Runtime Stage Security"
FROM node:18-alpine

# Create non-root user before copying files
RUN addgroup --system --gid 1001 nodejs && \
    adduser --system --uid 1001 astrojs

# Copy and set ownership
COPY --from=build /app/dist ./dist
COPY --from=build /app/node_modules ./node_modules/
RUN chown -R astrojs:nodejs /app

# Switch to non-root user
USER astrojs

# Application runs as astrojs user (uid 1001)
CMD ["node", "dist/server/entry.mjs"]
```

**Security Benefits**:
- **Principle of least privilege** - Container runs with minimal permissions
- **Attack surface reduction** - Compromised process has limited system access
- **Process isolation** - Each container has distinct user identity

### Container Security in Kubernetes

Kubernetes enhances container security through additional layers:

```yaml title="Pod Security Context"
securityContext:
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  fsGroup: 1001
  seccompProfile:
    type: RuntimeDefault
```

This ensures the pod:
- Cannot run as root (`runAsNonRoot: true`)
- Uses the same user/group IDs as the container
- Uses default seccomp profile for system call filtering

## Automated Dependency Management with Renovate

Keeping dependencies up-to-date is crucial for security and features. I use Renovate, configured via a GitHub Actions workflow [renovate.yaml](https://github.com/pipitonelabs/k8s-gitops/blob/main/.github/workflows/renovate.yaml), to automate this process.

### How It Works:
- **Scheduled Runs**: Executes hourly to check for updates.
- **PR Creation**: When new versions are detected, Renovate creates pull requests with changelogs.
- **Merging Triggers Deployment**: Once I merge a PR, the CI pipeline builds a new image, and Flux deploys it.

The workflow uses a GitHub App for authentication, ensuring secure access without personal tokens.

```yaml title=".github/workflows/renovate.yaml"
- name: Run Renovate
  uses: renovatebot/github-action@03026bd55840025343414baec5d9337c5f9c7ea7 # v44.0.4
  env:
    LOG_LEVEL: "${{ inputs.logLevel || 'debug' }}"
    RENOVATE_AUTODISCOVER: true
    RENOVATE_AUTODISCOVER_FILTER: "${{ github.repository }}"
```

This setup keeps my blog current with the latest Astro, Node.js, and other dependencies without manual intervention.

## Deployment with Helm and Flux

For deployment, I use Flux's HelmRelease powered by the [bjw-s-labs/app-template](https://github.com/bjw-s-labs/helm-charts) chart. This replaces writing full custom Helm charts for every app with concise `values.yaml` files (50-150 lines). It leverages the `bjw-s/common` library for best practices like security contexts, probes, Gateway API routes, and schema validation. A single chart upgrade propagates features and fixes across all my apps instantly, keeping my Git repo lean and drift-free.

The HelmRelease specifies the image tag (updated manually or via automation):

```yaml title="helmrelease.yaml"
containers:
  main:
    image:
      repository: ghcr.io/pipitonelabs/pipitonelabs.com
      tag: v1.0.7
```

Flux watches for new image tags and automatically updates the deployment. This GitOps approach ensures the cluster state matches the desired configuration.

## Networking and Exposure

Getting the blog accessible from the internet involves several layers:

### External-DNS, Envoy Gateway, and Cloudflare Tunnel

The HelmRelease configures the route directly:

```yaml title="helmrelease.yaml"
# Continued from HelmRelease above
  route:
    app:
      hostnames:
        - blog.pipitonelabs.com
      parentRefs:
        - name: envoy-external
          namespace: networking
```

External-DNS syncs the hostname to DNS, Envoy Gateway (via `envoy-external`) handles ingress routing, and Cloudflare Tunnel provides secure exposure without open ports.

### Envoy Gateway
Acts as the ingress controller built for the Gateway API, routing external traffic to internal services. It supports advanced features like TLS termination and load balancing.

### Cloudflare Tunnel
Provides secure, encrypted tunnels from my homelab to Cloudflare's edge. This avoids exposing ports directly on my home network, enhancing security.

Together, these tools create a seamless path from `https://blog.pipitonelabs.com` to my Kubernetes pods.

## Conclusion

This setup demonstrates the power of combining open-source tools for a robust, automated deployment pipeline. From local development to global access, every step is streamlined and secure.

The beauty of Kubernetes in a homelab is the learning opportunityâ€”it mirrors production environments while giving full control. If you're considering a similar setup, start small: get a basic deployment working, then layer on automation.

---

*Have questions about the setup? Drop a comment below.*