---
title: Deploying My Astro Blog in a Self-Hosted Kubernetes HomeLab
description: 'A comprehensive guide to deploying an Astro blog in a self-hosted Kubernetes HomeLab using GitHub Actions and Renovate.'
publishDate: 2025-11-29
updatedDate: 2025-12-11
tags:
  - Kubernetes
  - Astro
  - GitHub Actions
  - Renovate
  - Flux
  - DevOps
  - GitOps
  - Homelab
  - CI/CD
heroImage: { src: './k8s.png', color: '#1567c5ff' }
draft: false
enableComment: true
---

Deploying this Astro-based blog to a self-hosted Kubernetes cluster has been a fascinating deep dive into understanding how modern web frameworks actually work under the hood. What started as wanting to containerize a simple blog became an exploration of Astro's TypeScript integration and server-side rendering capabilities. As I've discovered, Astro's flexibility with different adapters (from Vercel's serverless functions to standalone Node.js) reveals how frameworks give developers the choice to control their deployment strategy. 

In this post, I'll walk you through my continuous deployment setup from code commits to public access via `blog.pipitonelabs.com`, and how I was able to get the solution to function in a containerized environment.

GitHub Actions builds container images which my homelab Kubernetes cluster then pulls, deploys, updates, and exposes securely. Here's the high-level flow:

1. **Code Changes**: Push to the main branch triggers automated builds and releases.
2. **Dependency Updates**: Renovate detects package updates and creates pull requests.
3. **Deployment**: Flux (via HelmRelease) deploys new images automatically.
4. **Networking**: External-DNS creates DNS records, Envoy Gateway exposes the service, and Cloudflare Tunnel secures external access.

## Continuous Integration with GitHub Actions

The heart of my CI/CD pipeline is the [build-k8s.yaml](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.github/workflows/build-k8s.yaml) GitHub Actions workflow. It uses semantic versioning to create releases based on commit messages, and builds & pushes the package to my GitHub container registry. There's some additional logic to add release details, depending on the commit type.

### Key Features:
- **Semantic Release**: Analyzes commit messages (e.g., `feat:`, `fix:`) to determine version bumps. Configured in [.releaserc.json](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.releaserc.json).
- **Docker Build**: Creates multi-platform images and pushes to GitHub Container Registry (GHCR) using the multi-stage [Dockerfile](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/Dockerfile).
- **Tagging Strategy**: Generates version tags like `v1.2.3`, `v1.2`, `v1`, and SHA-based tags for flexibility.

### Semantic Release in Action

Semantic release analyzes conventional commit messages per [.releaserc.json](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.releaserc.json):

| Commit | Example | Bump | Tags |
|--------|---------|------|------|
| feat | `feat: add dark mode` | minor | v1.2.0, v1.2, v1 |
| fix/perf/revert/refactor | `fix: resolve login bug` | patch | v1.1.1, v1.1, v1 |
| docs/style/chore/etc | `docs: update readme` | none | SHA only |

The workflow runs on pushes to `main` or can be manually triggered. It outputs image metadata for easy consumption by deployment tools.

```yaml title=".github/workflows/build-k8s.yaml"
jobs:
  release:
    name: Semantic Release & Build Image
    runs-on: ubuntu-latest
    outputs:
      new_release_published: ${{ steps.semantic.outputs.new_release_published }}
      new_release_version: ${{ steps.semantic.outputs.new_release_version }}
  build-and-push:
    name: Build and Push Docker Image
    needs: release
    if: needs.release.outputs.new_release_published == 'true'
```

This ensures only meaningful changes trigger new deployments, reducing unnecessary builds.

## From Slow Builds to Lightning-Fast Caching

When I first started containerizing my Astro blog, my builds were taking forever - probably 5+ minutes every time I made a tiny change to a file, or a blog post. That's when I realized I needed to understand how Docker caching actually works.

### Why Order Matters

Docker layers are like a stack of pancakes. Each instruction adds a new layer on top, and Docker is pretty smart about reusing layers when nothing has changed above them. But if you change something near the top, everything below gets rebuilt.

So I started with this simple question: "What changes most rarely in my project?"

The answer was obvious:
1. **Base system stuff** (Node, build tools) - rarely changes
2. **Dependencies** (package.json, bun.lock) - changes occasionally 
3. **My actual code** - changes constantly

By structuring my Dockerfile around this reality, I cut the build time just about in half - from 5-minutes to 2.5-minutes or less in some cases.

### The Build Stages Breakdown

The Dockerfile has four distinct stages, each serving a specific purpose:

**Base Stage**: This is where I install stuff that rarely changes - the Python and build tools needed for Sharp (required for importing images dynamically). Once this builds, it stays cached for a while.

**Dependencies Stage**: I copy just the package files first (`package.json`, `bun.lock`) and run `bun install`. This means when I update a dependency, only this stage and everything after it rebuilds.

**Build Stage**: This is where I copy in all my source code, modify the Astro config for Kubernetes (more on that in a sec), and run the build. This stage rebuilds most frequently but is still pretty fast.

**Runtime Stage**: This creates the final, lean image with just Alpine Linux and the essential runtime files. Small, secure, and production-ready.

### Build Performance

| What I Changed          | Before Optimization | After Caching |
|----------------------|----------------------|---------------|
| A CSS tweak          | Full rebuild (5m+)    | 2.5 minutes   |
| New blog post        | Full rebuild (5m+)    | 2.5 minutes   |

### Some Tweaks That Fixed Things

**Sharp Was a Pain** 

For the life of me, I could not figure out why certain images that were being dyamically imported were only working on my Vercel deployment, but not when I ran my application in Kubernetes. Sharp needed some build tools during the build process, so I split this across stages - build tools in the base stage, runtime `libvips` in the final stage. The `SHARP_IGNORE_GLOBAL_LIBVIPS=1` environment variable tells Sharp to use its own bundled libraries instead of fighting with system versions.

**The Adapter Switch Trick**

Here's something I'm pretty proud of. My blog uses Vercel's adapter locally (for easy development), but I needed to switch to a standalone Node adapter for Kubernetes. Instead of maintaining two different config files or using sed during the build (which was invalidating my cache), I implemented an environment-based approach:

```typescript title="astro.config.ts"
// Environment-based adapter selection for Vercel vs Docker/Kubernetes
const isDockerBuild = process.env.BUILD_TARGET === 'docker'
const adapter = isDockerBuild ? node({ mode: 'standalone' }) : vercel()
```

This approach is cleaner and more maintainable. The Dockerfile sets `BUILD_TARGET=docker` during the build stage, which automatically selects the Node adapter. For local development and Vercel deployments, the default Vercel adapter is used.

I also optimized the Docker build caching strategy using `.dockerignore` patterns:

```dockerfile title="Dockerfile"
# Copy only package files first (leverages Docker layer caching)
COPY package*.json bun.lock ./
# Copy full packages directory (needed for workspace linking)
COPY packages/ ./packages/

# Set build target to use Node adapter instead of Vercel
ENV BUILD_TARGET=docker
```

```bash title=".dockerignore"
# Optimization: exclude non-essential package files before dependency install
# These will be copied in the final COPY . . but won't invalidate dependency cache
packages/**/README.md
packages/**/LICENSE
packages/**/.npmignore
```

The workspace package `astro-pure` must be fully present during `bun install` for proper workspace linking and bin command resolution. Instead of trying to copy only `package.json` files (which breaks workspace functionality), I optimized the `.dockerignore` to exclude frequently-changing documentation files that don't affect dependencies. This means documentation updates won't invalidate the expensive dependency install cache layer.

This means one codebase works everywhere. I'm able to:

1. Push my commits to main
2. Trigger the Vercel deployment
3. Trigger [build-k8s.yaml](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.github/workflows/build-k8s.yaml) to create new version tags and build a new image
4. Next time Renovate runs, it creates a PR
5. After the merge completes, the new image is deployed in my Kubernetes cluster

The new approach is more efficient, maintains better cache utilization, and provides a cleaner separation of concerns.

**Shift-Left Security**

I built security into the process instead of bolting it on at the end. The `.dockerignore` file keeps sensitive stuff out of the build (no accidentally copying `.env` files or git history), and the final stage runs as a non-root user. Kubernetes enforces this further with pod security contexts.

The result? A nice reduction in build times, smaller images, and a setup that's secure by default.

## Additional Security Hardening

Security should be a primary concern in production containerization. Here are the critical practices I've implemented:

### .dockerignore File

A `.dockerignore` file prevents unnecessary files from being copied into the build context, reducing image size and attack surface:

```bash title=".dockerignore"
# Dependencies and build artifacts
node_modules/
dist/
.cache/

# Environment and secrets
.env*
!.env.example

# Git and CI/CD
.git/
.github/
.gitlab-ci.yml

# Documentation and development files
README.md
CHANGELOG.md
.vscode/
.prettierrc

# OS and editor files
.DS_Store
Thumbs.db
*.swp
```

**Benefits**:
- **Faster builds** - Less data to transfer and process
- **Smaller images** - Excludes unnecessary development files
- **Security** - Prevents accidentally including secrets or sensitive files

### Non-Root User Implementation

Running containers as root is a security anti-pattern. The runtime stage creates a dedicated user:

```dockerfile title="Dockerfile"
FROM node:18-alpine

# Create non-root user before copying files
RUN addgroup --system --gid 1001 nodejs && \
    adduser --system --uid 1001 astrojs

# Copy and set ownership
COPY --from=build /app/dist ./dist
COPY --from=build /app/node_modules ./node_modules/
RUN chown -R astrojs:nodejs /app

# Switch to non-root user
USER astrojs

# Application runs as astrojs user (uid 1001)
CMD ["node", "dist/server/entry.mjs"]
```

**Security Benefits**:
- **Principle of least privilege** - Container runs with minimal permissions
- **Attack surface reduction** - Compromised process has limited system access
- **Process isolation** - Each container has distinct user identity

### Container Security in Kubernetes

In addition to the Dockerfile's security, I updated my `helmrelease.yaml` to include a PodSecurityContext:

```yaml title="Pod Security Context"
securityContext:
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  fsGroup: 1001
  seccompProfile:
    type: RuntimeDefault
```

This ensures the pod:
- Cannot run as root (`runAsNonRoot: true`)
- Uses the same user/group IDs as the container
- Uses default seccomp profile for system call filtering

## Automated Dependency Management with Renovate

Keeping dependencies up-to-date is crucial for security and features. I use Renovate, configured via a GitHub Actions workflow [renovate.yaml](https://github.com/pipitonelabs/k8s-gitops/blob/main/.github/workflows/renovate.yaml), to automate this process.

### How It Works:
- **Scheduled Runs**: Executes hourly to check for updates.
- **PR Creation**: When new versions are detected, Renovate creates pull requests with changelogs.
- **Merging Triggers Deployment**: Once I merge a PR, the CI pipeline builds a new image, and Flux deploys it.

```yaml title=".github/workflows/renovate.yaml"
- name: Run Renovate
  uses: renovatebot/github-action@03026bd55840025343414baec5d9337c5f9c7ea7 # v44.0.4
  env:
    LOG_LEVEL: "${{ inputs.logLevel || 'debug' }}"
    RENOVATE_AUTODISCOVER: true
    RENOVATE_AUTODISCOVER_FILTER: "${{ github.repository }}"
```

This setup keeps my blog current with the latest release package with minimal manual intervention - I have to actually click the "Merge pull request" button after I complete my review.

## Kubernetes Deployment with Helm and Flux

For deployment, I use a [helmrelease.yaml](https://github.com/pipitonelabs/k8s-gitops/blob/main/kubernetes/apps/default/blog/app/helmrelease.yaml) file powered by the [bjw-s-labs/app-template](https://github.com/bjw-s-labs/helm-charts) chart. This replaces writing full custom Helm charts for every app with a `values.yaml` file which can be incorporated into the `helmrelease.yaml` file itself. A single chart upgrade propagates best practices, features and fixes across all my apps instantly.

The HelmRelease specifies the image tag (updated manually or via automation):

```yaml title="helmrelease.yaml"
containers:
  main:
    image:
      repository: ghcr.io/pipitonelabs/pipitonelabs.com
      tag: v1.0.7
```

Flux watches for new image tags and automatically updates the deployment. This GitOps approach ensures the cluster state matches the desired configuration.

## Networking and Exposure

Getting the blog accessible from the internet involves several layers:

### External-DNS, Envoy Gateway, and Cloudflare Tunnel

The HelmRelease configures the route directly:

```yaml title="helmrelease.yaml"
# Continued from HelmRelease above
  route:
    app:
      hostnames:
        - blog.pipitonelabs.com
      parentRefs:
        - name: envoy-external
          namespace: networking
```

External-DNS syncs the hostname to DNS, Envoy Gateway (via `envoy-external`) handles ingress routing, and Cloudflare Tunnel provides secure exposure without open ports.

### Envoy Gateway
Envoy Gateway is built on **Envoy Proxy** (the actual data plane) and implements the **Gateway API** specification. Think of it this way:

- **Envoy Proxy**: The high-performance L7 proxy that actually handles traffic filtering, load balancing, and routing
- **Gateway API**: The Kubernetes standard API for configuring ingress/gateway resources (replacing the old Ingress API)
- **Envoy Gateway**: The control plane that translates Gateway API configurations into Envoy Proxy configurations

I just write my routes in the HelmRelease like normal Kubernetes resources, and I can expose a service internally or externally.

### Cloudflare Tunnel
Provides secure, encrypted tunnels from my homelab to Cloudflare's edge. This avoids exposing ports directly on my home network, enhancing security.

Together, these tools create a seamless path from `https://blog.pipitonelabs.com` to my Kubernetes pods.

## Summary

This setup demonstrates the power of combining open-source tools for a robust, automated deployment pipeline. From local development to global access, every step is streamlined and secure.

The beauty of Kubernetes in a homelab is the learning opportunity - it mirrors production environments while giving full control. If you're considering a similar setup, start small: get a basic deployment working, then layer on automation.

---

*Have questions or want to point out mistakes, inaccuracies or improvements related to my setup? Drop a comment below.*